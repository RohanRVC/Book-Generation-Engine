{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2, 
   "metadata": {},
   "outputs": [], 
   "source": [
    "df=pd.read_csv(\"C:/Users/RohanRVC/Documents/Business/E-books/E_Books_Unknown_AI/Book/Dataset_collection/Updated_dataset/Final_data1.csv\")"
   ] 
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, pickle , numpy as np\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from tensorflow.keras.models import Sequential,load_model\n",
    "from tensorflow.keras.layers import LSTM,Dense,Activation \n",
    "## LSTM is RNN \n",
    "from tensorflow.keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=list(df.Content.values)\n",
    "joined_text=\" \".join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3784140608"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# partial_text=joined_text[:100000]\n",
    "len(joined_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks=100000\n",
    "for i in range(0,len(joined_text),chunks):\n",
    "    tokenizer=RegexpTokenizer(r\"\\w+\")\n",
    "    partial_text=joined_text[i:i+chunks]\n",
    "    tokens=tokenizer.tokenize(partial_text.lower())\n",
    "\n",
    "    # Load unique tokens and indices\n",
    "    with open('C:/Users/RohanRVC/Documents/Business/E-books/E_Books_Unknown_AI/text_generation/Model/model_stage_1/unique_tokens.pkl', 'rb') as f:\n",
    "        unique_tokens = pickle.load(f)\n",
    "\n",
    "    with open('C:/Users/RohanRVC/Documents/Business/E-books/E_Books_Unknown_AI/text_generation/Model/model_stage_1/unique_token_index.pkl', 'rb') as f:\n",
    "        unique_token_index = pickle.load(f)\n",
    "\n",
    "\n",
    "\n",
    "    # Update unique tokens and indices\n",
    "    unique_token = np.unique(tokens)\n",
    "    combined_tokens = set(unique_tokens).union(set(unique_token))\n",
    "    updated_unique_tokens = np.array(list(combined_tokens))\n",
    "    updated_unique_token_index = {token: idx for idx, token in enumerate(updated_unique_tokens)}\n",
    "\n",
    "    with open('C:/Users/RohanRVC/Documents/Business/E-books/E_Books_Unknown_AI/text_generation/Model/model_stage_1/unique_tokens.pkl', 'wb') as f:\n",
    "        pickle.dump(updated_unique_tokens, f)\n",
    "\n",
    "    # Save unique_token_index\n",
    "    with open('C:/Users/RohanRVC/Documents/Business/E-books/E_Books_Unknown_AI/text_generation/Model/model_stage_1/unique_token_index.pkl', 'wb') as f:\n",
    "        pickle.dump(updated_unique_token_index, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:   0%|          | 0/37842 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  97%|█████████▋| 36604/37842 [12:53:57<44:29,  2.16s/it]     "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from tqdm import tqdm\n",
    "# Assuming 'joined_text' is your entire text data loaded as a single string\n",
    "\n",
    "chunks = 100000  # Define the chunk size for processing\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# Initialize or load unique tokens and indices\n",
    "unique_tokens = set()\n",
    "unique_token_index = {}\n",
    " \n",
    "# Process text in chunks and update vocabulary\n",
    "for i in tqdm(range(0, len(joined_text), chunks), desc=\"Processing chunks\"):\n",
    "    partial_text = joined_text[i:i + chunks]\n",
    "    tokens = set(tokenizer.tokenize(partial_text.lower()))\n",
    "\n",
    "    # Update if there are new tokens\n",
    "    if tokens.difference(unique_tokens):\n",
    "        unique_tokens.update(tokens)\n",
    "        unique_token_index = {token: idx for idx, token in enumerate(unique_tokens)}\n",
    "\n",
    "        # Save updated unique tokens and indices\n",
    "        with open('unique_tokens.pkl', 'wb') as f:\n",
    "            pickle.dump(unique_tokens, f)\n",
    "        with open('unique_token_index.pkl', 'wb') as f:\n",
    "            pickle.dump(unique_token_index, f)\n",
    "\n",
    "# At this point, unique_tokens and unique_token_index are fully prepared for later use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('C:/Users/RohanRVC/Documents/Business/E-books/E_Books_Unknown_AI/text_generation/Model/model_stage_2/unique_tokens.pkl', 'rb') as f:\n",
    "        unique_tokens = pickle.load(f)\n",
    "\n",
    "with open('C:/Users/RohanRVC/Documents/Business/E-books/E_Books_Unknown_AI/text_generation/Model/model_stage_2/unique_token_index.pkl', 'rb') as f:\n",
    "        unique_token_index = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_tokens=list(unique_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_tokens.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'yancowinna'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=[1,2,3,4,5]\n",
    "a[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1853239"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unique_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Book",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
