{
 "cells": [  
  {
   "cell_type": "code", 
   "execution_count": 6,       
   "metadata": {},             
   "outputs": [],         
   "source": [              
    "import pandas as pd ,numpy as np"                 
   ]                   
  },            
  {     
   "cell_type": "code",      
   "execution_count": 7, 
   "metadata": {},   
   "outputs": [],  
   "source": [  
    "df=pd.read_csv(\"C:/Users/RohanRVC/Documents/Business/E-books/E_Books_Unknown_AI/Book/Dataset_collection/Updated_dataset/Final_data1.csv\")\n"  
   ] 
  }, 
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.rename(columns={'Langauge':'Language'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['Language'] = np.where(df['Language'] == 'en', 'English', df['Language'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Content</th>\n",
       "      <th>Language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Declaration of Independence of the United ...</td>\n",
       "      <td>History</td>\n",
       "      <td>All of the original files are included in the ...</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The King James Version of the Bible</td>\n",
       "      <td>Religion and Spirituality</td>\n",
       "      <td>Exodus The Third Book of Moses: Called Levitic...</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Complete Works of William Shakespeare</td>\n",
       "      <td>Drama and Performing Arts</td>\n",
       "      <td>William Shakespeare Contents THE SONNETS ALL’S...</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Magna Carta</td>\n",
       "      <td>Linguistics and Languages</td>\n",
       "      <td>preparer of the 0.1 version. some of which wer...</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Apocolocyntosis</td>\n",
       "      <td>Religion and Spirituality</td>\n",
       "      <td>H. D. Rouse Release date : November 1, ROUSE, ...</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11340</th>\n",
       "      <td>A Biographical Sketch of the Life and Characte...</td>\n",
       "      <td>Other</td>\n",
       "      <td>Produced by John Young Le Bourgeois A BIOGRAPH...</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11341</th>\n",
       "      <td>Rub iy t of Omar Khayy m, and Sal m n and Abs ...</td>\n",
       "      <td>Linguistics and Languages</td>\n",
       "      <td>by Edward FitzGerald. no cost and with almost ...</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11342</th>\n",
       "      <td>Jane Austen, Her Life and Letters: A Family Re...</td>\n",
       "      <td>History</td>\n",
       "      <td>Chapter I AUSTENS AND LEIGHS 1600-1764 At the ...</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11343</th>\n",
       "      <td>Mr  Dooley in Peace and in War</td>\n",
       "      <td>Comics and Graphic Novels</td>\n",
       "      <td>Graeme Mackreth, DOOLEY In Peace and in War Bo...</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11344</th>\n",
       "      <td>The Devil's Asteroid</td>\n",
       "      <td>Science Fiction</td>\n",
       "      <td>Extensive research did not uncover any evidenc...</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11345 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Title  \\\n",
       "0      The Declaration of Independence of the United ...   \n",
       "1                    The King James Version of the Bible   \n",
       "2              The Complete Works of William Shakespeare   \n",
       "3                                        The Magna Carta   \n",
       "4                                        Apocolocyntosis   \n",
       "...                                                  ...   \n",
       "11340  A Biographical Sketch of the Life and Characte...   \n",
       "11341  Rub iy t of Omar Khayy m, and Sal m n and Abs ...   \n",
       "11342  Jane Austen, Her Life and Letters: A Family Re...   \n",
       "11343                     Mr  Dooley in Peace and in War   \n",
       "11344                               The Devil's Asteroid   \n",
       "\n",
       "                           Genre  \\\n",
       "0                        History   \n",
       "1      Religion and Spirituality   \n",
       "2      Drama and Performing Arts   \n",
       "3      Linguistics and Languages   \n",
       "4      Religion and Spirituality   \n",
       "...                          ...   \n",
       "11340                      Other   \n",
       "11341  Linguistics and Languages   \n",
       "11342                    History   \n",
       "11343  Comics and Graphic Novels   \n",
       "11344            Science Fiction   \n",
       "\n",
       "                                                 Content Language  \n",
       "0      All of the original files are included in the ...  English  \n",
       "1      Exodus The Third Book of Moses: Called Levitic...  English  \n",
       "2      William Shakespeare Contents THE SONNETS ALL’S...  English  \n",
       "3      preparer of the 0.1 version. some of which wer...  English  \n",
       "4      H. D. Rouse Release date : November 1, ROUSE, ...  English  \n",
       "...                                                  ...      ...  \n",
       "11340  Produced by John Young Le Bourgeois A BIOGRAPH...  English  \n",
       "11341  by Edward FitzGerald. no cost and with almost ...  English  \n",
       "11342  Chapter I AUSTENS AND LEIGHS 1600-1764 At the ...  English  \n",
       "11343  Graeme Mackreth, DOOLEY In Peace and in War Bo...  English  \n",
       "11344  Extensive research did not uncover any evidenc...  English  \n",
       "\n",
       "[11345 rows x 4 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"C:/Users/RohanRVC/Documents/Business/E-books/E_Books_Unknown_AI/Book/Dataset_collection/Updated_dataset/Final_data1.csv\",index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM-based Text Generation Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2000  # Adjust based on your available memory\n",
    "start_row = 0\n",
    "vocab_size = 10000  # Adjust based on your dataset\n",
    "embedding_dim = 64\n",
    "max_length = 200  # Max length of sequences\n",
    "trunc_type = \"post\"\n",
    "padding_type = \"post\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\n",
    "model = Sequential([\n",
    "    Embedding(vocab_size, embedding_dim),\n",
    "    LSTM(64),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')  # Adjust the output layer based on your problem\n",
    "])\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])  # Adjust based on your problem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\RohanRVC\\Documents\\Business\\E-books\\E_Books_Unknown_AI\\04_new_model_training.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/RohanRVC/Documents/Business/E-books/E_Books_Unknown_AI/04_new_model_training.ipynb#X12sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m preprocessed_texts \u001b[39m=\u001b[39m df_chunk[\u001b[39m'\u001b[39m\u001b[39mContent\u001b[39m\u001b[39m'\u001b[39m]  \u001b[39m# Truncating text\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/RohanRVC/Documents/Business/E-books/E_Books_Unknown_AI/04_new_model_training.ipynb#X12sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m# Tokenize the text\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/RohanRVC/Documents/Business/E-books/E_Books_Unknown_AI/04_new_model_training.ipynb#X12sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m tokenizer\u001b[39m.\u001b[39;49mfit_on_texts(preprocessed_texts)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/RohanRVC/Documents/Business/E-books/E_Books_Unknown_AI/04_new_model_training.ipynb#X12sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m sequences \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mtexts_to_sequences(preprocessed_texts)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/RohanRVC/Documents/Business/E-books/E_Books_Unknown_AI/04_new_model_training.ipynb#X12sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m padded_sequences \u001b[39m=\u001b[39m pad_sequences(sequences, maxlen\u001b[39m=\u001b[39mmax_length, padding\u001b[39m=\u001b[39mpadding_type, truncating\u001b[39m=\u001b[39mtrunc_type)\n",
      "File \u001b[1;32mc:\\Users\\RohanRVC\\Documents\\Business\\E-books\\E_Books_Unknown_AI\\Book\\Lib\\site-packages\\keras\\src\\preprocessing\\text.py:293\u001b[0m, in \u001b[0;36mTokenizer.fit_on_texts\u001b[1;34m(self, texts)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    292\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39manalyzer \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 293\u001b[0m         seq \u001b[39m=\u001b[39m text_to_word_sequence(\n\u001b[0;32m    294\u001b[0m             text,\n\u001b[0;32m    295\u001b[0m             filters\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfilters,\n\u001b[0;32m    296\u001b[0m             lower\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlower,\n\u001b[0;32m    297\u001b[0m             split\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msplit,\n\u001b[0;32m    298\u001b[0m         )\n\u001b[0;32m    299\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    300\u001b[0m         seq \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39manalyzer(text)\n",
      "File \u001b[1;32mc:\\Users\\RohanRVC\\Documents\\Business\\E-books\\E_Books_Unknown_AI\\Book\\Lib\\site-packages\\keras\\src\\preprocessing\\text.py:74\u001b[0m, in \u001b[0;36mtext_to_word_sequence\u001b[1;34m(input_text, filters, lower, split)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Converts a text to a sequence of words (or tokens).\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \n\u001b[0;32m     48\u001b[0m \u001b[39mDeprecated: `tf.keras.preprocessing.text.text_to_word_sequence` does not\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[39m    A list of words (or tokens).\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     73\u001b[0m \u001b[39mif\u001b[39;00m lower:\n\u001b[1;32m---> 74\u001b[0m     input_text \u001b[39m=\u001b[39m input_text\u001b[39m.\u001b[39;49mlower()\n\u001b[0;32m     76\u001b[0m translate_dict \u001b[39m=\u001b[39m {c: split \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m filters}\n\u001b[0;32m     77\u001b[0m translate_map \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m\u001b[39m.\u001b[39mmaketrans(translate_dict)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'float' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    # Batch Load Data\n",
    "    df_chunk = pd.read_csv('C:/Users/RohanRVC/Documents/Business/E-books/E_Books_Unknown_AI/Book/Dataset_collection/Updated_dataset/Final_data1.csv', skiprows=range(1, start_row), nrows=batch_size)\n",
    "    \n",
    "    # Check if any data is left\n",
    "    if df_chunk.shape[0] == 0:\n",
    "        break\n",
    "    \n",
    "    # Preprocess the 'Content' column\n",
    "    preprocessed_texts = df_chunk['Content']  # Truncating text\n",
    "    \n",
    "    # Tokenize the text\n",
    "    tokenizer.fit_on_texts(preprocessed_texts)\n",
    "    sequences = tokenizer.texts_to_sequences(preprocessed_texts)\n",
    "    padded_sequences = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "    \n",
    "    # Create Labels (Assuming binary classification)\n",
    "    labels = np.array([1 if i % 2 == 0 else 0 for i in range(len(padded_sequences))])  # Replace with your actual labels\n",
    "    \n",
    "    # Train the Model\n",
    "    model.fit(padded_sequences, labels, epochs=1)\n",
    "    \n",
    "    # Update Start Row for the next batch\n",
    "    start_row += batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Building the model\n",
    "model = Sequential([\n",
    "    Embedding(vocab_size, 50, input_length=sequence_length),\n",
    "    LSTM(100, return_sequences=True),\n",
    "    LSTM(100),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dense(vocab_size, activation='softmax')\n",
    "])\n",
    "\n",
    "# 6. Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# 7. Fit the model\n",
    "model.fit(X, y, batch_size=128, epochs=10)\n",
    "\n",
    "# Save the model\n",
    "model.save('my_LSTM_model.h5')\n",
    "\n",
    "print(\"Model training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: 0, Driver Version: 528.24, GPU Memory Free: 5979.0MB\n"
     ]
    }
   ],
   "source": [
    "import GPUtil\n",
    "\n",
    "# Get the GPU details\n",
    "gpus = GPUtil.getGPUs()\n",
    "\n",
    "# Print detailed information for each GPU\n",
    "for gpu in gpus:\n",
    "    print(f\"ID: {gpu.id}, Driver Version: {gpu.driver}, GPU Memory Free: {gpu.memoryFree}MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Memory: 15 GB\n",
      "Available Memory: 4 GB\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "\n",
    "# Get the memory details\n",
    "memory = psutil.virtual_memory()\n",
    "\n",
    "# Total memory\n",
    "print(f\"Total Memory: {memory.total // (1024 ** 3)} GB\")\n",
    "\n",
    "# Available memory\n",
    "print(f\"Available Memory: {memory.available // (1024 ** 3)} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF Version:- 2.13.0\n",
      "TF hub Version:- 0.14.0\n",
      "GPU Not available :(\n"
     ]
    }
   ],
   "source": [
    "#Important necessary tools\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "print(\"TF Version:-\",tf.__version__)\n",
    "print(\"TF hub Version:-\",hub.__version__)\n",
    "\n",
    "#Check for GPU Availability\n",
    "print(\"GPU\",'available (YESSSSS!!!!!)' if tf.config.list_physical_devices(\"GPU\") else \"Not available :(\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow_hubNote: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.2.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Downloading tensorflow_hub-0.14.0-py2.py3-none-any.whl (90 kB)\n",
      "     ---------------------------------------- 90.3/90.3 kB 1.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.12.0 in c:\\users\\rohanrvc\\documents\\business\\e-books\\e_books_unknown_ai\\book\\lib\\site-packages (from tensorflow_hub) (1.24.3)\n",
      "Requirement already satisfied: protobuf>=3.19.6 in c:\\users\\rohanrvc\\documents\\business\\e-books\\e_books_unknown_ai\\book\\lib\\site-packages (from tensorflow_hub) (4.24.3)\n",
      "Installing collected packages: tensorflow_hub\n",
      "Successfully installed tensorflow_hub-0.14.0\n"
     ]
    }
   ],
   "source": [
    "# pip install tensorflow_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Book",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
